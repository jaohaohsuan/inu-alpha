view-thread-pool-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "fork-join-executor"
  # Configuration for the fork join pool
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 2
    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 2.0
    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  loglevel = debug
  log-dead-letters = 1
  log-dead-letters-during-shutdown = off

  actor {

    // notice:
    // We only recommend using the config option turned on when you're running tests.
    // It is completely pointless to have it turned on in other scenarios.

    //serialize-messages = on
    //serialize-creators = on

    kryo  {
      type = "graph"
      idstrategy = "automatic" //
      kryo-custom-serializer-init = "com.inu.cluster.KryoInit"
      kryo-trace = false
      implicit-registration-logging = true
    }

    serializers {
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }

    serialization-bindings {
      "com.inu.protocol.storedquery.messages.BoolClause" = kryo
      "com.inu.protocol.storedquery.messages.Command" = kryo
      "com.inu.protocol.storedquery.messages.Ack" = kryo
      "com.inu.protocol.storedquery.messages.Event" = kryo
    }

    provider = "akka.cluster.ClusterActorRefProvider"
    debug {
      receive = off
      lifecycle = off
    }
  }

  remote {
    transport-failure-detector {
      heartbeat-interval = 30s
      acceptable-heartbeat-pause = 10s
    }
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = 127.0.0.1
      port = 2551
    }
  }

  cluster {
    log-info = off

    metrics.enabled = off
    client.receptionist {
      name = receptionist
    }
    seed-nodes = []
    min-nr-of-members = 2
  }

  http {
    host-connection-pool {
      max-retries = 5
    }
  }

  extensions = [
    "akka.cluster.metrics.ClusterMetricsExtension",
    "akka.cluster.client.ClusterClientReceptionist",
    "com.romix.akka.serialization.kryo.KryoSerializationExtension$",
  ]
}

akka.cluster.singleton {
  # The actor name of the child singleton actor.
  singleton-name = "singleton"

  # Singleton among the nodes tagged with specified role.
  # If the role is not specified it's a singleton among all nodes in the cluster.
  # role = "compute"
}

akka.cluster.singleton-proxy {
  # The actor name of the singleton actor that is started by the ClusterSingletonManager
  singleton-name = ${akka.cluster.singleton.singleton-name}

  # The role of the cluster nodes where the singleton can be deployed.
  # If the role is not specified then any node will do.
  # role = "compute"

  buffer-size = 1000
}

storedq {
  cluster-name = "storedq"
  cassandra-nodes = ""
  cassandra-nodes = ${?CASSANDRA_NODES}
}

akka.persistence.journal.plugin = "cassandra-journal"
akka.persistence.snapshot-store.plugin = "cassandra-snapshot-store"

cassandra-journal.refresh-interval = 500ms

elasticsearch {
  transport-address = "127.0.0.1"
  transport-address = ${?ES_TRANSPORT_ADDRESS}
  client-address = "127.0.0.1"
  client-address = ${?ES_CLIENT_ADDRESS}
  transport-tcp = 9300
  transport-tcp = ${?ES_TRANSPORT_TCP}
  client-http = 9200
  client-http = ${?ES_CLIENT_HTTP}
  cluster-name = "elasticsearch"
  cluster-name = ${?ES_CLUSTER_NAME}
}

spray.routing {
  users {
    atlas = subaru
    dev = grandsys
  }
}