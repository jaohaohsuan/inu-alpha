service {
  port = 7879
  host = "0.0.0.0"
  user-profile {
    host = "http://127.0.0.1:2403"
    host = ${?USER_PROFILE_HOST}
    filter = "users/me?include=datasourcesRangeEsQuery"
    filter = ${?USER_PROFILE_FILTER}
    filters = "users/me?include=datasourcesRangeEsQueries"
    filters = ${?USER_PROFILE_FILTERS}
  }
  dapi {
    host = "http://127.0.0.1:2403"
    host = ${?DAPI_HOST}
    logsfilter = "logsfilter"
    logsfilter = ${?DAPI_LOGSFILTER}
  }
  cors {
    allow_headers = [
      "Accept",
      "X-Real-IP",
      "X-Forwarded-For",
      "X-Forwarded-Prefix",
      "Accept-Language",
      "Accept-Encoding",
      "Authoriaztion",
      "Host",
      "Origin",
      "Referer",
      "User-Agent",
      "Location",
      "Content-Type",
      "X-Requested-With",
      "X-HTTP-Method-Overrid",
      "client_ip",
      "uid"
    ]
    expose_headers = [
      "Location"
    ]
    allow_credentials = true
    allow_origin = "*"
  }
}

my-thread-pool-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "thread-pool-executor"
  # Configuration for the thread pool
  thread-pool-executor {
    # minimum number of threads to cap factor-based core number to
    core-pool-size-min = 2
    # No of core threads ... ceil(available processors * factor)
    core-pool-size-factor = 2.0
    # maximum number of threads to cap factor-based number to
    core-pool-size-max = 10
  }
  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 100
}

akka {
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
  log-dead-letters = 1
  log-dead-letters-during-shutdown = off

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"

    kryo {
      type = "graph"
      implicit-registration-logging = true
      kryo-trace = false
      kryo-custom-serializer-init = "com.inu.protocol.Serialization.KryoInit"
      idstrategy = "automatic"
    }

    serializers {
      kryo = "com.romix.akka.serialization.kryo.KryoSerializer"
    }

    serialization-bindings {
      "com.inu.protocol.storedquery.messages.BoolClause" = kryo
      "com.inu.protocol.storedquery.messages.Command" = kryo
      "com.inu.protocol.storedquery.messages.Ack" = kryo
      "com.inu.protocol.storedquery.messages.Event" = kryo
    }
  }

  remote {
    transport-failure-detector {
      heartbeat-interval = 30s
      acceptable-heartbeat-pause = 10s
    }
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = 127.0.0.1
      port = 2552
    }
  }

  cluster {
    client.receptionist {
      name = receptionist
    }
    seed-nodes = []
    roles = [ "frontend" ]
  }

  extensions = [
    "akka.cluster.metrics.ClusterMetricsExtension",
    "com.romix.akka.serialization.kryo.KryoSerializationExtension$",
    "akka.cluster.client.ClusterClientReceptionist"
  ]
}

storedq {
  cluster-name = "storedq"
}

elasticsearch {
  transport-address = "127.0.0.1"
  transport-address = ${?ES_TRANSPORT_ADDRESS}
  client-address = "127.0.0.1"
  client-address = ${?ES_CLIENT_ADDRESS}
  transport-tcp = 9300
  transport-tcp = ${?ES_TRANSPORT_TCP}
  client-http = 9200
  client-http = ${?ES_CLIENT_HTTP}
  cluster-name = "elasticsearch"
  cluster-name = ${?ES_CLUSTER_NAME}
}
